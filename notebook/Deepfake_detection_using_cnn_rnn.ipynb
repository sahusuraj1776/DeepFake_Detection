{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 12739\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"C:/Users/Sahu Suraj/Videos/New Project/DeepFake Detection/\"\n",
    "TRAIN_SAMPLE_FOLDER = \"DATASET\"\n",
    "print(f\"Train Samples: {len(os.listdir(os.path.join(DATA_FOLDER,TRAIN_SAMPLE_FOLDER)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>owxbbpjpch.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wynotylpnm.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpmyeepbep.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fzvpbrzssi.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>htorvhbcae.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wclvkepakb.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fckxaqjbxk.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vpmyeepbep.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  split        original\n",
       "owxbbpjpch.mp4  FAKE  train  wynotylpnm.mp4\n",
       "vpmyeepbep.mp4  REAL  train             NaN\n",
       "fzvpbrzssi.mp4  REAL  train             NaN\n",
       "htorvhbcae.mp4  FAKE  train  wclvkepakb.mp4\n",
       "fckxaqjbxk.mp4  FAKE  train  vpmyeepbep.mp4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_metadata = pd.read_json(\"C:/Users/Sahu Suraj/Videos/New Project/DeepFake Detection/DATASET/metadata.json\").T\n",
    "train_sample_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sample_metadata = train_sample_metadata.reset_index().rename(columns={'index': 'Video'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>owxbbpjpch.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wynotylpnm.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpmyeepbep.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fzvpbrzssi.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>htorvhbcae.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wclvkepakb.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fckxaqjbxk.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vpmyeepbep.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  split        original\n",
       "owxbbpjpch.mp4  FAKE  train  wynotylpnm.mp4\n",
       "vpmyeepbep.mp4  REAL  train             NaN\n",
       "fzvpbrzssi.mp4  REAL  train             NaN\n",
       "htorvhbcae.mp4  FAKE  train  wclvkepakb.mp4\n",
       "fckxaqjbxk.mp4  FAKE  train  vpmyeepbep.mp4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12738, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "FAKE    11016\n",
      "REAL     1722\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_sample_metadata.groupby('label')['label'].count().plot(figsize=(15,5),kind='bar',title=\"Distribution of Labels in the Training Set\")\n",
    "plt.show()\n",
    "print(train_sample_metadata.groupby('label')['label'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = train_sample_metadata[train_sample_metadata[\"label\"] == \"REAL\"]\n",
    "fake_df = train_sample_metadata[train_sample_metadata[\"label\"] == \"FAKE\"]\n",
    "# sample_size = 1722\n",
    "\n",
    "real_df = real_df.sample(1722, random_state=42)\n",
    "fake_df = fake_df.sample(1722, random_state=42)\n",
    "\n",
    "train_sample_metadata = pd.concat([real_df, fake_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3444, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMsAAAHqCAYAAAAapcmWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDyklEQVR4nO3debxVdb0//tdhOgzCUUA4kIBkDihoqcVgCoiCKFKiGdHlSil6c4qQSu0aYF0xG7Q0zQxFBdN7S9TCL+WQ03XWSCGvYaFoccSMWQOE/fujxf55ZBDw4EF9Ph+P9fCsz/qsz3qvvffBzYvPWquiVCqVAgAAAACkQX0XAAAAAADbC2EZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQDUoSlTpqSioqK8NG3aNNXV1enfv38mTZqUhQsXrrfPhAkTUlFRsUXHee211zJhwoTcc889W7Tfho616667ZsiQIVs0ztu54YYbcskll2xwW0VFRSZMmFCnx6trd911Vw488MC0aNEiFRUVueWWWzbY7/nnn09FRUW+973v1clx+/Xrl+7du9fJWG8es1+/fnUyVl2/d7fffvtGx6uoqMjpp59eZ8d6q3W/C2+31MVr905et7p8/7bU6tWrc+WVV+bjH/94WrdunebNm6dLly751Kc+lenTp2/VmBdccMFGf58AYHvRqL4LAID3o2uuuSZ77bVXVq9enYULF+aBBx7Id77znXzve9/LTTfdlMMOO6zc96STTsoRRxyxReO/9tprmThxYpJs0V+kt+ZYW+OGG27I7NmzM2bMmPW2PfTQQ9lll122eQ1bq1Qq5fjjj88ee+yR2267LS1atMiee+5Z32VtF+r6vbv99tvz4x//uF7C07f+LixYsCDDhg3LGWeckREjRpTbW7Vq9Y6P9U5et8svv/wdH39rjRw5MjfffHPGjBmTiRMnprKyMn/5y18yc+bM/OY3v8kxxxyzxWNecMEFOe644/LpT3+67gsGgDoiLAOAbaB79+458MADy+vHHntsvvKVr+STn/xkhg0blrlz56Z9+/ZJkl122WWbh0evvfZamjdv/q4c6+306tWrXo//dv72t7/lH//4R4455pgMGDCgvsvZrmzv792WeOvvwvPPP58k6dy58ybPc/Xq1amoqEijRpv/NfqdvG577733Vu/7TsybNy833XRTvvnNb5aD+SQZMGBARo8enbVr19ZLXQDwbnAZJgC8Szp37pzvf//7WbZsWa688spy+4Yujbz77rvTr1+/tGnTJs2aNUvnzp1z7LHH5rXXXsvzzz+fnXfeOUkyceLE8uVio0aNqjXek08+meOOOy477bRTdtttt40ea53p06dn3333TdOmTfPhD384P/rRj2ptX3eJ6bpQYZ177rknFRUV5UtC+/XrlxkzZuSFF16odTnbOhu6JG327Nn51Kc+lZ122ilNmzbNRz/60Vx77bUbPM7Pf/7zfOMb30jHjh3TqlWrHHbYYXn22Wc3/sK/yQMPPJABAwakZcuWad68efr06ZMZM2aUt0+YMKEcoHz9619PRUVFdt11180ae1N+/OMf55BDDkm7du3SokWL9OjRIxdddFFWr169wf73339/evXqlWbNmuVDH/pQzjvvvKxZs6ZWn1WrVuXb3/529tprr1RWVmbnnXfOF77whbzyyitvW88VV1yR/fbbLzvssENatmyZvfbaK+eee+7b7vfW927dZ+J3v/tdvvSlL6Vt27Zp06ZNhg0blr/97W+bHGvUqFH58Y9/XB533fLWz9f111+fbt26pXnz5tlvv/3y61//er2x5s6dmxEjRqRdu3aprKxMt27dymO/E+s+c9dff33OOuusfOhDH0plZWWee+65vPLKKzn11FOz9957Z4cddki7du1y6KGH5v77719vnHfyur31Msw3X/r7gx/8IF27ds0OO+yQ3r175+GHH17v2FdddVX22GOPVFZWZu+9984NN9yQUaNGve3n+tVXX02SdOjQYYPbGzSo/deIpUuXZty4cenatWuaNGmSD33oQxkzZkxWrFhR63VYsWJFrr322jq9zBUA6pqZZQDwLjryyCPTsGHD3HfffRvt8/zzz+eoo47KwQcfnKuvvjo77rhj/vrXv2bmzJlZtWpVOnTokJkzZ+aII47IiSeemJNOOilJygHaOsOGDcvw4cPzH//xH7X+wrohs2bNypgxYzJhwoRUV1dn2rRp+fKXv5xVq1Zl3LhxW3SOl19+eU4++eT8+c9/3qz7Gj377LPp06dP2rVrlx/96Edp06ZNpk6dmlGjRuXll1/O1772tVr9zz333Bx00EH52c9+lqVLl+brX/96jj766DzzzDNp2LDhRo9z77335vDDD8++++6byZMnp7KyMpdffnmOPvro/PznP89nP/vZnHTSSdlvv/1qXY5XWVm5Ree/IX/+858zYsSIcpDwhz/8If/1X/+V//u//8vVV19dq29NTU2GDx+es88+O+eff35mzJiRb3/721m0aFEuu+yyJMnatWvzqU99Kvfff3++9rWvpU+fPnnhhRcyfvz49OvXL48//niaNWu2wVpuvPHGnHrqqTnjjDPyve99Lw0aNMhzzz2XP/7xj1t9fieddFKOOuqo3HDDDXnxxRfz1a9+Nf/2b/+Wu+++e6P7nHfeeVmxYkV+8Ytf5KGHHiq3vzmcmTFjRh577LGcf/752WGHHXLRRRflmGOOybPPPpsPf/jDSZI//vGP6dOnTzmMrq6uzm9+85uceeaZ+fvf/57x48dv9Xmtc84556R37975yU9+kgYNGqRdu3blUHL8+PGprq7O8uXLM3369PTr1y933XXXZoVAW/O6rfPjH/84e+21V/negOedd16OPPLIzJs3L1VVVUmSn/70pznllFNy7LHH5uKLL86SJUsyceLErFy58m3H79atW3bcccdMnDgxDRo0yMCBAzcasL322mvp27dvXnrppZx77rnZd999M2fOnHzzm9/M008/nTvvvDMVFRV56KGHcuihh6Z///4577zzktTNZa4AUOdKAECdueaaa0pJSo899thG+7Rv377UrVu38vr48eNLb/5f8i9+8YtSktKsWbM2OsYrr7xSSlIaP378etvWjffNb35zo9verEuXLqWKior1jnf44YeXWrVqVVqxYkWtc5s3b16tfr/73e9KSUq/+93vym1HHXVUqUuXLhus/a11Dx8+vFRZWVmaP39+rX6DBw8uNW/evLR48eJaxznyyCNr9fvv//7vUpLSQw89tMHjrdOrV69Su3btSsuWLSu3vfHGG6Xu3buXdtlll9LatWtLpVKpNG/evFKS0ne/+91NjrelfddZs2ZNafXq1aXrrruu1LBhw9I//vGP8ra+ffuWkpRuvfXWWvuMHj261KBBg9ILL7xQKpVKpZ///OelJKVf/vKXtfo99thjpSSlyy+/vNaYffv2La+ffvrppR133HGz632zt7536z4Tp556aq1+F110USlJacGCBZsc77TTTlvv8/jmY7Vv3760dOnScltNTU2pQYMGpUmTJpXbBg0aVNpll11KS5YsqbX/6aefXmratGmt13dTNvRervvMHXLIIW+7/xtvvFFavXp1acCAAaVjjjlmvXPZ2tftre/fujp79OhReuONN8rtjz76aClJ6ec//3mpVPrX56y6urrUs2fPWsd44YUXSo0bN97o7+ebzZgxo9S2bdtSklKSUps2bUqf+cxnSrfddlutfpMmTSo1aNBgvT/31v1Zdvvtt5fbWrRoUTrhhBPe9tgAUJ9chgkA77JSqbTJ7R/96EfTpEmTnHzyybn22mvzl7/8ZauOc+yxx25233322Sf77bdfrbYRI0Zk6dKlefLJJ7fq+Jvr7rvvzoABA9KpU6da7aNGjcprr71Wa9ZRkgwdOrTW+r777pskeeGFFzZ6jBUrVuSRRx7Jcccdlx122KHc3rBhw4wcOTIvvfTSZl/KuTV+//vfZ+jQoWnTpk0aNmyYxo0b59///d+zZs2a/OlPf6rVt2XLluud44gRI7J27dryjMRf//rX2XHHHXP00UfnjTfeKC8f/ehHU11dvcmnpH7iE5/I4sWL87nPfS633npr/v73v7/j89ua92Rz9O/fPy1btiyvt2/fPu3atSuP+89//jN33XVXjjnmmDRv3rzWa3HkkUfmn//85wYvTdxSG/td+slPfpL9998/TZs2TaNGjdK4cePcddddeeaZZzZr3Hfyuh111FG1ZlK+dd9nn302NTU1Of7442vt17lz5xx00EGbVd+RRx6Z+fPnZ/r06Rk3blz22Wef3HLLLRk6dGitJ5X++te/Tvfu3fPRj3601nswaNCgWpdoA8B7hbAMAN5FK1asyKuvvpqOHTtutM9uu+2WO++8M+3atctpp52W3XbbLbvttlt++MMfbtGxNnavoQ2prq7eaNu6exdtK6+++uoGa133Gr31+G3atKm1vu4yyddff32jx1i0aFFKpdIWHaeuzJ8/PwcffHD++te/5oc//GHuv//+PPbYY+V7ar217nUPfnizt74XL7/8chYvXpwmTZqkcePGtZaamppNBmAjR47M1VdfnRdeeCHHHnts2rVrl549e+aOO+7Y6nPcmvdka8ZdN/a6cV999dW88cYbufTSS9d7HY488sgkqZMwcEOfmx/84Af50pe+lJ49e+aXv/xlHn744Tz22GM54ogjNvu838nr9nb7rvusbOjztKG2jWnWrFk+/elP57vf/W7uvffePPfcc9l7773z4x//OHPmzEnyr8/jU089td570LJly5RKpTp5DwDg3eSeZQDwLpoxY0bWrFnztvczOvjgg3PwwQdnzZo1efzxx3PppZdmzJgxad++fYYPH75Zx9rYjfw3pKamZqNt6/5S3rRp0yRZ735H7/Qvwm3atMmCBQvWa193o/O2bdu+o/GTZKeddkqDBg22+XE25JZbbsmKFSty8803p0uXLuX2WbNmbbD/yy+/vF7bW9+LdTeEnzlz5gbHePNsrA35whe+kC984QtZsWJF7rvvvowfPz5DhgzJn/70p1o1bu922mmn8uzA0047bYN9unbt+o6Ps6HfpalTp6Zfv3654oorarUvW7bsHR+vLqz7rGzq87Q1OnfunJNPPjljxozJnDlzss8++6Rt27Zp1qzZevffW2db/W4BwLYiLAOAd8n8+fMzbty4VFVV5ZRTTtmsfRo2bJiePXtmr732yrRp0/Lkk09m+PDhdTZzZ505c+bkD3/4Q61LMW+44Ya0bNky+++/f5KUb+791FNPZc899yz3u+2229Yb782zf97OgAEDMn369Pztb3+rNePuuuuuS/PmzdOrV6+tOaVaWrRokZ49e+bmm2/O9773vfLN79euXZupU6dml112yR577PGOj7Mh64KWNz8ooFQq5aqrrtpg/2XLluW2226rdYneDTfckAYNGuSQQw5JkgwZMiQ33nhj1qxZk549e251bS1atMjgwYOzatWqfPrTn86cOXPe1bDszZ/jjT2QYFOaN2+e/v375/e//3323XffNGnSpK5L3KiKior1Hv7w1FNP5aGHHlrvkuL6sOeee6a6ujr//d//nbFjx5bb58+fnwcffHCTs1uTf30OKyoqal22vM66y0zXjTFkyJBccMEFadOmzduGk1vyZwMA1BdhGQBsA7Nnzy7ft2fhwoW5//77c80116Rhw4aZPn36ek+ufLOf/OQnufvuu3PUUUelc+fO+ec//1mesXHYYYcl+dfMoS5duuTWW2/NgAED0rp167Rt23ajT6t7Ox07dszQoUMzYcKEdOjQIVOnTs0dd9yR73znO2nevHmS5OMf/3j23HPPjBs3Lm+88UZ22mmnTJ8+PQ888MB64/Xo0SM333xzrrjiihxwwAFp0KBBDjzwwA0ee/z48fn1r3+d/v3755vf/GZat26dadOmZcaMGbnooovKT/Z7pyZNmpTDDz88/fv3z7hx49KkSZNcfvnlmT17dn7+859v0Uy8t3r66afzi1/8Yr32j3/84zn88MPTpEmTfO5zn8vXvva1/POf/8wVV1yRRYsWbXCsNm3a5Etf+lLmz5+fPfbYI7fffnuuuuqqfOlLX0rnzp2TJMOHD8+0adNy5JFH5stf/nI+8YlPpHHjxnnppZfyu9/9Lp/61KdyzDHHbHD80aNHp1mzZjnooIPSoUOH1NTUZNKkSamqqsrHP/7xrX4NtkaPHj2SJN/5zncyePDgNGzYcItDrx/+8If55Cc/mYMPPjhf+tKXsuuuu2bZsmV57rnn8qtf/Wqzniy5NYYMGZJvfetbGT9+fPr27Ztnn302559/frp27Zo33nhjmxxzSzRo0CATJ07MKaeckuOOOy5f/OIXs3jx4kycODEdOnRIgwabvhvLs88+m0GDBmX48OHp27dvOnTokEWLFmXGjBn56U9/mn79+qVPnz5JkjFjxuSXv/xlDjnkkHzlK1/Jvvvum7Vr12b+/Pn57W9/m7POOqsc6vbo0SP33HNPfvWrX6VDhw5p2bJlrfAdALYHwjIA2Aa+8IUvJEmaNGmSHXfcMd26dcvXv/71nHTSSZsMypJ/3eD/t7/9bcaPH5+amprssMMO6d69e2677bYMHDiw3G/y5Mn56le/mqFDh2blypU54YQTMmXKlK2q96Mf/Wi+8IUvZPz48Zk7d246duyYH/zgB/nKV75S7tOwYcP86le/yumnn57/+I//SGVlZYYPH57LLrssRx11VK3xvvzlL2fOnDk599xzs2TJkpRKpY0+2GDPPffMgw8+mHPPPTennXZaXn/99XTr1i3XXHNNRo0atVXnsyF9+/bN3XffnfHjx2fUqFFZu3Zt9ttvv9x2220ZMmTIOxr7uuuuy3XXXbde+7pz+OUvf5n//M//zLBhw9KmTZuMGDEiY8eOzeDBg9fbp7q6Oj/+8Y8zbty4PP3002ndunXOPffcTJw4sdynYcOGue222/LDH/4w119/fSZNmpRGjRpll112Sd++fcsh1IYcfPDBmTJlSv77v/87ixYtStu2bfPJT34y11133dt+NuvaiBEj8r//+7+5/PLLc/7556dUKmXevHlbFPruvffeefLJJ/Otb30r//mf/5mFCxdmxx13zO67716+b9m28I1vfCOvvfZaJk+enIsuuih77713fvKTn2T69OnbzQ3tTz755FRUVOSiiy7KMccck1133TVnn312br311syfP3+T+37kIx/J2LFjc/fdd+fWW2/NK6+8ksaNG2f33XfPt7/97YwdO7YcuLVo0SL3339/Lrzwwvz0pz/NvHnz0qxZs3Tu3DmHHXZYrffzhz/8YU477bQMHz48r732Wvr27bvdvF4AsE5F6e0eyQUAALwvLF68OHvssUc+/elP56c//Wl9lwMA2yUzywAA4H2opqYm//Vf/5X+/funTZs2eeGFF3LxxRdn2bJl+fKXv1zf5QHAdktYBgAA70OVlZV5/vnnc+qpp+Yf//hH+YEZP/nJT7LPPvvUd3kAsN1yGSYAAAAAFDb9GBwAAAAA+AARlgEAAABAQVgGAAAAAIX37Q3+165dm7/97W9p2bJlKioq6rscAAAAAOpRqVTKsmXL0rFjxzRosPH5Y+/bsOxvf/tbOnXqVN9lAAAAALAdefHFF7PLLrtsdPv7Nixr2bJlkn+9AK1atarnagAAAACoT0uXLk2nTp3KmdHGvG/DsnWXXrZq1UpYBgAAAECSvO3tutzgHwAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAApbHJbdd999Ofroo9OxY8dUVFTklltuqbW9oqJig8t3v/vdcp9+/fqtt3348OG1xlm0aFFGjhyZqqqqVFVVZeTIkVm8ePFWnSQAAAAAbI4tDstWrFiR/fbbL5dddtkGty9YsKDWcvXVV6eioiLHHntsrX6jR4+u1e/KK6+stX3EiBGZNWtWZs6cmZkzZ2bWrFkZOXLklpYLAAAAAJut0ZbuMHjw4AwePHij26urq2ut33rrrenfv38+/OEP12pv3rz5en3XeeaZZzJz5sw8/PDD6dmzZ5LkqquuSu/evfPss89mzz333NKyAQAAAOBtbdN7lr388suZMWNGTjzxxPW2TZs2LW3bts0+++yTcePGZdmyZeVtDz30UKqqqspBWZL06tUrVVVVefDBBzd4rJUrV2bp0qW1FgAAAADYEls8s2xLXHvttWnZsmWGDRtWq/3zn/98unbtmurq6syePTvnnHNO/vCHP+SOO+5IktTU1KRdu3brjdeuXbvU1NRs8FiTJk3KxIkT6/4kAAAAAPjA2KZh2dVXX53Pf/7zadq0aa320aNHl3/u3r17dt999xx44IF58skns//++yf514MC3qpUKm2wPUnOOeecjB07try+dOnSdOrUqS5OAwAAAIAPiG0Wlt1///159tlnc9NNN71t3/333z+NGzfO3Llzs//++6e6ujovv/zyev1eeeWVtG/ffoNjVFZWprKy8h3XDQAAAMAH1za7Z9nkyZNzwAEHZL/99nvbvnPmzMnq1avToUOHJEnv3r2zZMmSPProo+U+jzzySJYsWZI+ffpsq5IBAAAA+IDb4plly5cvz3PPPVdenzdvXmbNmpXWrVunc+fOSf51CeT//M//5Pvf//56+//5z3/OtGnTcuSRR6Zt27b54x//mLPOOisf+9jHctBBByVJunXrliOOOCKjR4/OlVdemSQ5+eSTM2TIEE/CBAAAAGCb2eKZZY8//ng+9rGP5WMf+1iSZOzYsfnYxz6Wb37zm+U+N954Y0qlUj73uc+tt3+TJk1y1113ZdCgQdlzzz1z5plnZuDAgbnzzjvTsGHDcr9p06alR48eGThwYAYOHJh99903119//dacIwAAAABslopSqVSq7yK2haVLl6aqqipLlixJq1at6rsc3sN2PXtGfZcAfMA9f+FR9V0CgO9EQL3znYh3anOzom12zzIAAAAAeK8RlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBhi8Oy++67L0cffXQ6duyYioqK3HLLLbW2jxo1KhUVFbWWXr161eqzcuXKnHHGGWnbtm1atGiRoUOH5qWXXqrVZ9GiRRk5cmSqqqpSVVWVkSNHZvHixVt8ggAAAACwubY4LFuxYkX222+/XHbZZRvtc8QRR2TBggXl5fbbb6+1fcyYMZk+fXpuvPHGPPDAA1m+fHmGDBmSNWvWlPuMGDEis2bNysyZMzNz5szMmjUrI0eO3NJyAQAAAGCzNdrSHQYPHpzBgwdvsk9lZWWqq6s3uG3JkiWZPHlyrr/++hx22GFJkqlTp6ZTp0658847M2jQoDzzzDOZOXNmHn744fTs2TNJctVVV6V379559tlns+eee25p2QAAAADwtrbJPcvuueeetGvXLnvssUdGjx6dhQsXlrc98cQTWb16dQYOHFhu69ixY7p3754HH3wwSfLQQw+lqqqqHJQlSa9evVJVVVXuAwAAAAB1bYtnlr2dwYMH5zOf+Uy6dOmSefPm5bzzzsuhhx6aJ554IpWVlampqUmTJk2y00471dqvffv2qampSZLU1NSkXbt2643drl27cp+3WrlyZVauXFleX7p0aR2eFQAAAAAfBHUeln32s58t/9y9e/cceOCB6dKlS2bMmJFhw4ZtdL9SqZSKiory+pt/3lifN5s0aVImTpz4DioHAAAA4INum1yG+WYdOnRIly5dMnfu3CRJdXV1Vq1alUWLFtXqt3DhwrRv377c5+WXX15vrFdeeaXc563OOeecLFmypLy8+OKLdXwmAAAAALzfbfOw7NVXX82LL76YDh06JEkOOOCANG7cOHfccUe5z4IFCzJ79uz06dMnSdK7d+8sWbIkjz76aLnPI488kiVLlpT7vFVlZWVatWpVawEAAACALbHFl2EuX748zz33XHl93rx5mTVrVlq3bp3WrVtnwoQJOfbYY9OhQ4c8//zzOffcc9O2bdscc8wxSZKqqqqceOKJOeuss9KmTZu0bt0648aNS48ePcpPx+zWrVuOOOKIjB49OldeeWWS5OSTT86QIUM8CRMAAACAbWaLw7LHH388/fv3L6+PHTs2SXLCCSfkiiuuyNNPP53rrrsuixcvTocOHdK/f//cdNNNadmyZXmfiy++OI0aNcrxxx+f119/PQMGDMiUKVPSsGHDcp9p06blzDPPLD81c+jQobnsssu2+kQBAAAA4O1UlEqlUn0XsS0sXbo0VVVVWbJkiUsyeUd2PXtGfZcAfMA9f+FR9V0CgO9EQL3znYh3anOzom1+zzIAAAAAeK8QlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAAhS0Oy+67774cffTR6dixYyoqKnLLLbeUt61evTpf//rX06NHj7Ro0SIdO3bMv//7v+dvf/tbrTH69euXioqKWsvw4cNr9Vm0aFFGjhyZqqqqVFVVZeTIkVm8ePFWnSQAAAAAbI4tDstWrFiR/fbbL5dddtl621577bU8+eSTOe+88/Lkk0/m5ptvzp/+9KcMHTp0vb6jR4/OggULysuVV15Za/uIESMya9aszJw5MzNnzsysWbMycuTILS0XAAAAADZboy3dYfDgwRk8ePAGt1VVVeWOO+6o1XbppZfmE5/4RObPn5/OnTuX25s3b57q6uoNjvPMM89k5syZefjhh9OzZ88kyVVXXZXevXvn2WefzZ577rmlZQMAAADA29rm9yxbsmRJKioqsuOOO9ZqnzZtWtq2bZt99tkn48aNy7Jly8rbHnrooVRVVZWDsiTp1atXqqqq8uCDD27wOCtXrszSpUtrLQAAAACwJbZ4ZtmW+Oc//5mzzz47I0aMSKtWrcrtn//859O1a9dUV1dn9uzZOeecc/KHP/yhPCutpqYm7dq1W2+8du3apaamZoPHmjRpUiZOnLhtTgQAAACAD4RtFpatXr06w4cPz9q1a3P55ZfX2jZ69Ojyz927d8/uu++eAw88ME8++WT233//JElFRcV6Y5ZKpQ22J8k555yTsWPHlteXLl2aTp061cWpAAAAAPABsU3CstWrV+f444/PvHnzcvfdd9eaVbYh+++/fxo3bpy5c+dm//33T3V1dV5++eX1+r3yyitp3779BseorKxMZWVlndQPAAAAwAdTnd+zbF1QNnfu3Nx5551p06bN2+4zZ86crF69Oh06dEiS9O7dO0uWLMmjjz5a7vPII49kyZIl6dOnT12XDAAAAABJtmJm2fLly/Pcc8+V1+fNm5dZs2aldevW6dixY4477rg8+eST+fWvf501a9aU7zHWunXrNGnSJH/+858zbdq0HHnkkWnbtm3++Mc/5qyzzsrHPvaxHHTQQUmSbt265Ygjjsjo0aNz5ZVXJklOPvnkDBkyxJMwAQAAANhmtjgse/zxx9O/f//y+rr7hJ1wwgmZMGFCbrvttiTJRz/60Vr7/e53v0u/fv3SpEmT3HXXXfnhD3+Y5cuXp1OnTjnqqKMyfvz4NGzYsNx/2rRpOfPMMzNw4MAkydChQ3PZZZdt8QkCAAAAwOba4rCsX79+KZVKG92+qW1J0qlTp9x7771ve5zWrVtn6tSpW1oeAAAAAGy1Or9nGQAAAAC8VwnLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoLDFYdl9992Xo48+Oh07dkxFRUVuueWWWttLpVImTJiQjh07plmzZunXr1/mzJlTq8/KlStzxhlnpG3btmnRokWGDh2al156qVafRYsWZeTIkamqqkpVVVVGjhyZxYsXb/EJAgAAAMDm2uKwbMWKFdlvv/1y2WWXbXD7RRddlB/84Ae57LLL8thjj6W6ujqHH354li1bVu4zZsyYTJ8+PTfeeGMeeOCBLF++PEOGDMmaNWvKfUaMGJFZs2Zl5syZmTlzZmbNmpWRI0duxSkCAAAAwOZptKU7DB48OIMHD97gtlKplEsuuSTf+MY3MmzYsCTJtddem/bt2+eGG27IKaeckiVLlmTy5Mm5/vrrc9hhhyVJpk6dmk6dOuXOO+/MoEGD8swzz2TmzJl5+OGH07NnzyTJVVddld69e+fZZ5/NnnvuubXnCwAAAAAbVaf3LJs3b15qamoycODAcltlZWX69u2bBx98MEnyxBNPZPXq1bX6dOzYMd27dy/3eeihh1JVVVUOypKkV69eqaqqKvd5q5UrV2bp0qW1FgAAAADYEnUaltXU1CRJ2rdvX6u9ffv25W01NTVp0qRJdtppp032adeu3Xrjt2vXrtznrSZNmlS+v1lVVVU6der0js8HAAAAgA+WbfI0zIqKilrrpVJpvba3emufDfXf1DjnnHNOlixZUl5efPHFragcAAAAgA+yOg3Lqqurk2S92V8LFy4szzarrq7OqlWrsmjRok32efnll9cb/5VXXllv1to6lZWVadWqVa0FAAAAALZEnYZlXbt2TXV1de64445y26pVq3LvvfemT58+SZIDDjggjRs3rtVnwYIFmT17drlP7969s2TJkjz66KPlPo888kiWLFlS7gMAAAAAdW2Ln4a5fPnyPPfcc+X1efPmZdasWWndunU6d+6cMWPG5IILLsjuu++e3XffPRdccEGaN2+eESNGJEmqqqpy4okn5qyzzkqbNm3SunXrjBs3Lj169Cg/HbNbt2454ogjMnr06Fx55ZVJkpNPPjlDhgzxJEwAAAAAtpktDssef/zx9O/fv7w+duzYJMkJJ5yQKVOm5Gtf+1pef/31nHrqqVm0aFF69uyZ3/72t2nZsmV5n4svvjiNGjXK8ccfn9dffz0DBgzIlClT0rBhw3KfadOm5cwzzyw/NXPo0KG57LLLtvpEAQAAAODtVJRKpVJ9F7EtLF26NFVVVVmyZIn7l/GO7Hr2jPouAfiAe/7Co+q7BADfiYB65zsR79TmZkXb5GmYAAAAAPBeJCwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAQp2HZbvuumsqKirWW0477bQkyahRo9bb1qtXr1pjrFy5MmeccUbatm2bFi1aZOjQoXnppZfqulQAAAAAqKXOw7LHHnssCxYsKC933HFHkuQzn/lMuc8RRxxRq8/tt99ea4wxY8Zk+vTpufHGG/PAAw9k+fLlGTJkSNasWVPX5QIAAABAWaO6HnDnnXeutX7hhRdmt912S9++fcttlZWVqa6u3uD+S5YsyeTJk3P99dfnsMMOS5JMnTo1nTp1yp133plBgwbVdckAAAAAkGQb37Ns1apVmTp1ar74xS+moqKi3H7PPfekXbt22WOPPTJ69OgsXLiwvO2JJ57I6tWrM3DgwHJbx44d07179zz44IMbPdbKlSuzdOnSWgsAAAAAbIltGpbdcsstWbx4cUaNGlVuGzx4cKZNm5a777473//+9/PYY4/l0EMPzcqVK5MkNTU1adKkSXbaaadaY7Vv3z41NTUbPdakSZNSVVVVXjp16rRNzgkAAACA9686vwzzzSZPnpzBgwenY8eO5bbPfvaz5Z+7d++eAw88MF26dMmMGTMybNiwjY5VKpVqzU57q3POOSdjx44try9dulRgBgAAAMAW2WZh2QsvvJA777wzN9988yb7dejQIV26dMncuXOTJNXV1Vm1alUWLVpUa3bZwoUL06dPn42OU1lZmcrKyropHgAAAIAPpG12GeY111yTdu3a5aijjtpkv1dffTUvvvhiOnTokCQ54IAD0rhx4/JTNJNkwYIFmT179ibDMgAAAAB4p7bJzLK1a9fmmmuuyQknnJBGjf7/QyxfvjwTJkzIsccemw4dOuT555/Pueeem7Zt2+aYY45JklRVVeXEE0/MWWedlTZt2qR169YZN25cevToUX46JgAAAABsC9skLLvzzjszf/78fPGLX6zV3rBhwzz99NO57rrrsnjx4nTo0CH9+/fPTTfdlJYtW5b7XXzxxWnUqFGOP/74vP766xkwYECmTJmShg0bbotyAQAAACDJNgrLBg4cmFKptF57s2bN8pvf/OZt92/atGkuvfTSXHrppduiPAAAAADYoG12zzIAAAAAeK8RlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEChzsOyCRMmpKKiotZSXV1d3l4qlTJhwoR07NgxzZo1S79+/TJnzpxaY6xcuTJnnHFG2rZtmxYtWmTo0KF56aWX6rpUAAAAAKhlm8ws22effbJgwYLy8vTTT5e3XXTRRfnBD36Qyy67LI899liqq6tz+OGHZ9myZeU+Y8aMyfTp03PjjTfmgQceyPLlyzNkyJCsWbNmW5QLAAAAAEmSRttk0EaNas0mW6dUKuWSSy7JN77xjQwbNixJcu2116Z9+/a54YYbcsopp2TJkiWZPHlyrr/++hx22GFJkqlTp6ZTp0658847M2jQoG1RMgAAAABsm5llc+fOTceOHdO1a9cMHz48f/nLX5Ik8+bNS01NTQYOHFjuW1lZmb59++bBBx9MkjzxxBNZvXp1rT4dO3ZM9+7dy30AAAAAYFuo85llPXv2zHXXXZc99tgjL7/8cr797W+nT58+mTNnTmpqapIk7du3r7VP+/bt88ILLyRJampq0qRJk+y0007r9Vm3/4asXLkyK1euLK8vXbq0rk4JAAAAgA+IOg/LBg8eXP65R48e6d27d3bbbbdce+216dWrV5KkoqKi1j6lUmm9trd6uz6TJk3KxIkT30HlAAAAAHzQbZPLMN+sRYsW6dGjR+bOnVu+j9lbZ4gtXLiwPNusuro6q1atyqJFizbaZ0POOeecLFmypLy8+OKLdXwmAAAAALzfbfOwbOXKlXnmmWfSoUOHdO3aNdXV1bnjjjvK21etWpV77703ffr0SZIccMABady4ca0+CxYsyOzZs8t9NqSysjKtWrWqtQAAAADAlqjzyzDHjRuXo48+Op07d87ChQvz7W9/O0uXLs0JJ5yQioqKjBkzJhdccEF233337L777rngggvSvHnzjBgxIklSVVWVE088MWeddVbatGmT1q1bZ9y4cenRo0f56ZgAAAAAsC3UeVj20ksv5XOf+1z+/ve/Z+edd06vXr3y8MMPp0uXLkmSr33ta3n99ddz6qmnZtGiRenZs2d++9vfpmXLluUxLr744jRq1CjHH398Xn/99QwYMCBTpkxJw4YN67pcAAAAACirKJVKpfouYltYunRpqqqqsmTJEpdk8o7sevaM+i4B+IB7/sKj6rsEAN+JgHrnOxHv1OZmRdv8nmUAAAAA8F4hLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKAjLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACnUelk2aNCkf//jH07Jly7Rr1y6f/vSn8+yzz9bqM2rUqFRUVNRaevXqVavPypUrc8YZZ6Rt27Zp0aJFhg4dmpdeeqmuywUAAACAsjoPy+69996cdtppefjhh3PHHXfkjTfeyMCBA7NixYpa/Y444ogsWLCgvNx+++21to8ZMybTp0/PjTfemAceeCDLly/PkCFDsmbNmrouGQAAAACSJI3qesCZM2fWWr/mmmvSrl27PPHEEznkkEPK7ZWVlamurt7gGEuWLMnkyZNz/fXX57DDDkuSTJ06NZ06dcqdd96ZQYMG1XXZAAAAALDt71m2ZMmSJEnr1q1rtd9zzz1p165d9thjj4wePToLFy4sb3viiSeyevXqDBw4sNzWsWPHdO/ePQ8++OAGj7Ny5cosXbq01gIAAAAAW2KbhmWlUiljx47NJz/5yXTv3r3cPnjw4EybNi133313vv/97+exxx7LoYcempUrVyZJampq0qRJk+y00061xmvfvn1qamo2eKxJkyalqqqqvHTq1GnbnRgAAAAA70t1fhnmm51++ul56qmn8sADD9Rq/+xnP1v+uXv37jnwwAPTpUuXzJgxI8OGDdvoeKVSKRUVFRvcds4552Ts2LHl9aVLlwrMAAAAANgi22xm2RlnnJHbbrstv/vd77LLLrtssm+HDh3SpUuXzJ07N0lSXV2dVatWZdGiRbX6LVy4MO3bt9/gGJWVlWnVqlWtBQAAAAC2RJ2HZaVSKaeffnpuvvnm3H333enatevb7vPqq6/mxRdfTIcOHZIkBxxwQBo3bpw77rij3GfBggWZPXt2+vTpU9clAwAAAECSbXAZ5mmnnZYbbrght956a1q2bFm+x1hVVVWaNWuW5cuXZ8KECTn22GPToUOHPP/88zn33HPTtm3bHHPMMeW+J554Ys4666y0adMmrVu3zrhx49KjR4/y0zEBAAAAoK7VeVh2xRVXJEn69etXq/2aa67JqFGj0rBhwzz99NO57rrrsnjx4nTo0CH9+/fPTTfdlJYtW5b7X3zxxWnUqFGOP/74vP766xkwYECmTJmShg0b1nXJAAAAAJBkG4RlpVJpk9ubNWuW3/zmN287TtOmTXPppZfm0ksvravSAAAAAGCTttkN/gEAAADgvUZYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAAVhGQAAAAAUhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYBAAAAQEFYBgAAAAAFYRkAAAAAFIRlAAAAAFAQlgEAAABAQVgGAAAAAIXtPiy7/PLL07Vr1zRt2jQHHHBA7r///vouCQAAAID3qe06LLvpppsyZsyYfOMb38jvf//7HHzwwRk8eHDmz59f36UBAAAA8D60XYdlP/jBD3LiiSfmpJNOSrdu3XLJJZekU6dOueKKK+q7NAAAAADehxrVdwEbs2rVqjzxxBM5++yza7UPHDgwDz744Hr9V65cmZUrV5bXlyxZkiRZunTpti2U9721K1+r7xKADzj/LwO2B74TAfXNdyLeqXWfoVKptMl+221Y9ve//z1r1qxJ+/bta7W3b98+NTU16/WfNGlSJk6cuF57p06dtlmNAPBuqLqkvisAAKh/vhNRV5YtW5aqqqqNbt9uw7J1Kioqaq2XSqX12pLknHPOydixY8vra9euzT/+8Y+0adNmg/0B3g1Lly5Np06d8uKLL6ZVq1b1XQ4AQL3wnQjYHpRKpSxbtiwdO3bcZL/tNixr27ZtGjZsuN4ssoULF6432yxJKisrU1lZWattxx133JYlAmy2Vq1a+WIIAHzg+U4E1LdNzShbZ7u9wX+TJk1ywAEH5I477qjVfscdd6RPnz71VBUAAAAA72fb7cyyJBk7dmxGjhyZAw88ML17985Pf/rTzJ8/P//xH/9R36UBAAAA8D60XYdln/3sZ/Pqq6/m/PPPz4IFC9K9e/fcfvvt6dKlS32XBrBZKisrM378+PUuEwcA+CDxnQh4L6kovd3zMgEAAADgA2K7vWcZAAAAALzbhGUAAAAAUBCWAQAAAEBBWAYAAAAABWEZAAAAABSEZQAAAABQEJYB1JGFCxducvsbb7yRRx999F2qBgBg+/Tiiy/mi1/8Yn2XAbBRwjKAOtKhQ4dagVm3bt0yf/788vqrr76a3r1710dpAADbjX/84x+59tpr67sMgI0SlgHUkVKpVGv9pZdeyhtvvLHJPgAAAGxfhGUA76KKior6LgEAAIBNEJYBAAAAQKFRfRcA8H5RUVGRZcuWpWnTpimVSqmoqMjy5cuzdOnSJCn/FwDg/WzYsGGb3L548eJ3pxCArVRRcgMdgDrRoEGDWpdZrgvM3rq+Zs2a+igPAOBd8YUvfGGz+l1zzTXbuBKArSMsA6gj995772b169u37zauBAAAgK0lLAOoI6tXr07jxo032Wf27Nnp3r37u1QRAMD2Ze3atZkxY0YmT56cW265pb7LAdggN/gHqCOf+9znsql/f5g9e3YGDBjwLlYEALB9mDt3bs4555zssssuOf744+u7HIBNEpYB1JFHHnkkp5xyyga3zZkzJwMGDMghhxzyLlcFAFA/Xn/99Vx77bU55JBDss8+++Siiy7K2WefnVdeecWsMmC7JiwDqCO//e1vM3369Jx99tm12p955pkMGDAgBx10UG688cZ6qg4A4N3x6KOP5uSTT051dXUuu+yyHHvssXnxxRfToEGDHHbYYdlhhx3qu0SATWpU3wUAvF9069Ytt99+ewYMGJA2bdrkq1/9av7v//4vhx56aHr27Jn/+Z//ScOGDeu7TACAbapPnz4544wz8uijj2bPPfes73IAtpiwDKAOffzjH88tt9ySIUOGZMWKFbnqqqty4IEH5he/+IWgDAD4QDj00EMzefLkLFy4MCNHjsygQYNSUVFR32UBbDZhGUAdO/TQQ3PDDTfkM5/5TAYOHJibb775bZ+SCQDwfvHb3/42L774Yq655pp86Utfyuuvv57PfvazSSI0A94TKkqbenQbAJttp512qvUFcNmyZWnWrFkaNar97xL/+Mc/3u3SAADqzR133JGrr746t9xySzp16pTjjjsuxx13XPbff//6Lg1gg4RlAHXk2muv3ax+J5xwwjauBABg+7No0aJMnTo1V199dZ566qmsWbOmvksC2CBhGcC76I033lhvphkAwAfNk08+aWYZsN1qUN8FAHwQ/PGPf8xZZ52VD33oQ/VdCgDANnXRRRfl9ddfL6/fd999WblyZXl92bJl+dnPflYfpQFsFjPLALaR5cuX58Ybb8zkyZPz2GOPpVevXjn22GPzla98pb5LAwDYZho2bJgFCxakXbt2SZJWrVpl1qxZ+fCHP5wkefnll9OxY0eXYQLbLdcCAdSxBx54ID/72c/yy1/+Ml27ds0f//jH3HvvvTnooIPquzQAgG3urfMxzM8A3mtchglQRy666KLstddeGT58eHbeeec88MADeeqpp1JRUZGddtqpvssDAABgM5hZBlBHzj333Hz961/P+eefn4YNG9Z3OQAAAGwFYRlAHTn//PMzZcqUXH/99fnc5z6XkSNHpnv37vVdFgDAu+5nP/tZdthhhyT/ehr4lClT0rZt2yT/usE/wPbMDf4B6ti9996bq6++Or/85S+z2267Zc6cOe5ZBgB8YOy6666pqKh4237z5s17F6oB2HLCMoA68pe//CVdu3YtfzlctmxZpk2blmuuuSZPPPFEPvGJT+S4447L2LFj67lSAID69de//jUf+tCH6rsMgA1yg3+AOrL77rvnlVdeKa+fdNJJOeaYY/LII4/k97//fT7xiU/kwgsvrMcKAQDqV01NTc4888x85CMfqe9SADZKWAZQR946Uff222/PihUrkiQ9evTIJZdckr/+9a/1URoAwLtm8eLF+fznP5+dd945HTt2zI9+9KOsXbs23/zmN/PhD384Dz30UK6++ur6LhNgo9zgH+Bd1Lhx4/ouAQBgmzr33HNz33335YQTTsjMmTPzla98JTNnzsw///nP/L//9//St2/f+i4RYJOEZQB1pKKiYr2b2W7OzW0BAN5PZsyYkWuuuSaHHXZYTj311HzkIx/JHnvskUsuuaS+SwPYLG7wD1BHGjRokMGDB6eysjJJ8qtf/SqHHnpoWrRoUavfzTffXB/lAQC8Kxo3bpwXXnghHTt2TJI0b948jz76aLp3717PlQFsHjPLAOrICSecUGv93/7t3+qpEgCA+rN27dpat55o2LDhev94CLA9M7MMAACAOmO2PfBeZ2YZAAAAdcZse+C9zswyAAAAACg0qO8CAAAAAGB7ISwDAAAAgIKwDAAAAAAKwjIAgPeAfv36ZcyYMZvV95577klFRUUWL178jo6566675pJLLnlHYwAAvNcIywAAAACgICwDAAAAgIKwDADgPWbq1Kk58MAD07Jly1RXV2fEiBFZuHDhev3+93//N/vtt1+aNm2anj175umnn661/cEHH8whhxySZs2apVOnTjnzzDOzYsWKd+s0AAC2S8IyAID3mFWrVuVb3/pW/vCHP+SWW27JvHnzMmrUqPX6ffWrX833vve9PPbYY2nXrl2GDh2a1atXJ0mefvrpDBo0KMOGDctTTz2Vm266KQ888EBOP/30d/lsAAC2L43quwAAALbMF7/4xfLPH/7wh/OjH/0on/jEJ7J8+fLssMMO5W3jx4/P4YcfniS59tprs8suu2T69Ok5/vjj893vfjcjRowoPzRg9913z49+9KP07ds3V1xxRZo2bfqunhMAwPbCzDIAgPeY3//+9/nUpz6VLl26pGXLlunXr1+SZP78+bX69e7du/xz69ats+eee+aZZ55JkjzxxBOZMmVKdthhh/IyaNCgrF27NvPmzXvXzgUAYHtjZhkAwHvIihUrMnDgwAwcODBTp07NzjvvnPnz52fQoEFZtWrV2+5fUVGRJFm7dm1OOeWUnHnmmev16dy5c53XDQDwXiEsAwB4D/m///u//P3vf8+FF16YTp06JUkef/zxDfZ9+OGHy8HXokWL8qc//Sl77bVXkmT//ffPnDlz8pGPfOTdKRwA4D3CZZgAAO8hnTt3TpMmTXLppZfmL3/5S2677bZ861vf2mDf888/P3fddVdmz56dUaNGpW3btvn0pz+dJPn617+ehx56KKeddlpmzZqVuXPn5rbbbssZZ5zxLp4NAMD2R1gGAPAesvPOO2fKlCn5n//5n+y999658MIL873vfW+DfS+88MJ8+ctfzgEHHJAFCxbktttuS5MmTZIk++67b+69997MnTs3Bx98cD72sY/lvPPOS4cOHd7N0wEA2O5UlEqlUn0XAQAAAADbAzPLAAAAAKAgLAMAAACAgrAMAAAAAArCMgAAAAAoCMsAAAAAoCAsAwAAAICCsAwAAAAACsIyAAAAACgIywAAAACgICwDAAAAgIKwDAAAAAAKwjIAAAAAKPx/8Y2HEvo9ZaIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "FAKE    1722\n",
      "REAL    1722\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_sample_metadata.groupby('label')['label'].count().plot(figsize=(15,5),kind='bar',title=\"Distribution of Labels in the Training Set\")\n",
    "plt.show()\n",
    "print(train_sample_metadata.groupby('label')['label'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "import keras\n",
    "from imutils import paths\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "def crop_center_square(frame):\n",
    "    y,x = frame.shape[0:2]\n",
    "    min_dim = min(y,x)\n",
    "    start_x = (x//2) - (min_dim//2)\n",
    "    start_y = (y//2) - (min_dim//2)\n",
    "    return frame[start_y: start_y + min_dim, start_x: start_x + min_dim]\n",
    "\n",
    "def load_video(path,max_frames=0,resize=(IMG_SIZE,IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame,resize)\n",
    "            frame = frame[:,:,[2,1,0]]\n",
    "            frames.append(frame)\n",
    "            \n",
    "            if len(frames) ==max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        pooling = \"avg\",\n",
    "        input_shape = (IMG_SIZE,IMG_SIZE,3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "    \n",
    "    inputs = keras.Input((IMG_SIZE,IMG_SIZE,3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "    \n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs,outputs,name='feature_extractor')\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAKE', 'REAL']\n"
     ]
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0,vocabulary=np.unique(train_sample_metadata.label))\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['REAL', 'REAL', 'REAL', ..., 'FAKE', 'FAKE', 'FAKE'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_sample_metadata.label.values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = label_processor(labels[...,None]).numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 200\n",
    "NUM_FEATURES =2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df,root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = list(df.index)\n",
    "    labels = df['label'].values\n",
    "    labels = label_processor(labels[...,None]).numpy()\n",
    "    \n",
    "    frame_masks= np.zeros(shape=(num_samples,MAX_SEQ_LENGTH),dtype='bool')\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples,MAX_SEQ_LENGTH,NUM_FEATURES),dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir,path))\n",
    "        frames = frames[None,...]\n",
    "        \n",
    "        temp_frame_mask = np.zeros(shape=(1,MAX_SEQ_LENGTH,),dtype='bool')\n",
    "        temp_frame_features = np.zeros(\n",
    "        shape=(1,MAX_SEQ_LENGTH,NUM_FEATURES),dtype=\"float32\"\n",
    "        )\n",
    "        \n",
    "        for i,batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH,video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i,j,:] = feature_extractor.predict(batch[None,j,:])\n",
    "            temp_frame_mask[i,:length] =1\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "    return (frame_features,frame_masks),labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2755, 3) (689, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_set , Test_set = train_test_split(train_sample_metadata,test_size = 0.2,random_state=42,stratify=train_sample_metadata['label'])\n",
    "\n",
    "print(Train_set.shape,Test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_labels = prepare_all_videos(Train_set,'train')\n",
    "test_data,test_labels = prepare_all_videos(Test_set,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (2755, 200, 2048)\n",
      "Frame masks in train set: (689, 200)\n",
      "Train labels in train set: (2755, 1)\n",
      "Test labels in test set: (689, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {test_data[1].shape}\")\n",
    "print(f\"Train labels in train set: {train_labels.shape}\")\n",
    "print(f\"Test labels in test set: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "REAL Train Videos: 1378\n",
      "FAKE Train Videos: 1377\n",
      "REAL Test Videos: 344\n",
      "FAKE Test Videos: 345\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][1066][0][2047])\n",
    "# print(train_data[1][27])\n",
    "count = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i]==1:\n",
    "        count += 1\n",
    "print(\"REAL Train Videos:\",count)\n",
    "count = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i]==0:\n",
    "        count += 1\n",
    "print(\"FAKE Train Videos:\",count)\n",
    "count=0\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i]==1:\n",
    "        count += 1\n",
    "print(\"REAL Test Videos:\",count)\n",
    "count = 0\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i]==0:\n",
    "        count += 1\n",
    "print(\"FAKE Test Videos:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.4754 - loss: 0.6932\n",
      "Epoch 1: val_loss improved from inf to 0.69315, saving model to ./hello/.weights.h5\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 389ms/step - accuracy: 0.4756 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 2/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.4854 - loss: 0.6932\n",
      "Epoch 2: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 285ms/step - accuracy: 0.4853 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 3/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.5212 - loss: 0.6932\n",
      "Epoch 3: val_loss improved from 0.69315 to 0.69315, saving model to ./hello/.weights.h5\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 298ms/step - accuracy: 0.5209 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 4/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.4881 - loss: 0.6932\n",
      "Epoch 4: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 301ms/step - accuracy: 0.4882 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 5/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.4926 - loss: 0.6932\n",
      "Epoch 5: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 288ms/step - accuracy: 0.4925 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 6/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.4872 - loss: 0.6932\n",
      "Epoch 6: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 306ms/step - accuracy: 0.4871 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 7/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.5099 - loss: 0.6931\n",
      "Epoch 7: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 346ms/step - accuracy: 0.5097 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 8/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.4928 - loss: 0.6932\n",
      "Epoch 8: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 372ms/step - accuracy: 0.4929 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 9/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.5258 - loss: 0.6931\n",
      "Epoch 9: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 336ms/step - accuracy: 0.5254 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 10/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.5050 - loss: 0.6931\n",
      "Epoch 10: val_loss improved from 0.69315 to 0.69315, saving model to ./hello/.weights.h5\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 305ms/step - accuracy: 0.5046 - loss: 0.6931 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 11/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.5068 - loss: 0.6932\n",
      "Epoch 11: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 265ms/step - accuracy: 0.5066 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 12/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.4987 - loss: 0.6932\n",
      "Epoch 12: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 279ms/step - accuracy: 0.4987 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 13/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.4951 - loss: 0.6932\n",
      "Epoch 13: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.4950 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 14/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.4819 - loss: 0.6932\n",
      "Epoch 14: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 279ms/step - accuracy: 0.4819 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 15/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.4946 - loss: 0.6932\n",
      "Epoch 15: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.4946 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 16/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.5173 - loss: 0.6930\n",
      "Epoch 16: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 280ms/step - accuracy: 0.5170 - loss: 0.6930 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 17/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.5013 - loss: 0.6932\n",
      "Epoch 17: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 276ms/step - accuracy: 0.5012 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 18/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.4910 - loss: 0.6932\n",
      "Epoch 18: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 269ms/step - accuracy: 0.4911 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 19/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.4886 - loss: 0.6932\n",
      "Epoch 19: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 280ms/step - accuracy: 0.4887 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 20/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.4895 - loss: 0.6932\n",
      "Epoch 20: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 265ms/step - accuracy: 0.4896 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 21/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.5273 - loss: 0.6931\n",
      "Epoch 21: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 281ms/step - accuracy: 0.5269 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 22/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.5033 - loss: 0.6932\n",
      "Epoch 22: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5033 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 23/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.5041 - loss: 0.6932\n",
      "Epoch 23: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 283ms/step - accuracy: 0.5040 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 24/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.5137 - loss: 0.6932\n",
      "Epoch 24: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 280ms/step - accuracy: 0.5132 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 25/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.5045 - loss: 0.6931\n",
      "Epoch 25: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 281ms/step - accuracy: 0.5045 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 26/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.4957 - loss: 0.6932\n",
      "Epoch 26: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 297ms/step - accuracy: 0.4958 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 27/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.4932 - loss: 0.6932\n",
      "Epoch 27: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 275ms/step - accuracy: 0.4933 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 28/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.5046 - loss: 0.6931\n",
      "Epoch 28: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 276ms/step - accuracy: 0.5045 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 29/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.4609 - loss: 0.6934\n",
      "Epoch 29: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 267ms/step - accuracy: 0.4611 - loss: 0.6934 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 30/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.5100 - loss: 0.6931\n",
      "Epoch 30: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 289ms/step - accuracy: 0.5099 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 31/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.5059 - loss: 0.6931\n",
      "Epoch 31: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 272ms/step - accuracy: 0.5055 - loss: 0.6931 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 32/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.5098 - loss: 0.6932\n",
      "Epoch 32: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 279ms/step - accuracy: 0.5097 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 33/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.4988 - loss: 0.6932\n",
      "Epoch 33: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 282ms/step - accuracy: 0.4988 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 34/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.5175 - loss: 0.6931\n",
      "Epoch 34: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 265ms/step - accuracy: 0.5172 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 35/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.4906 - loss: 0.6932\n",
      "Epoch 35: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 289ms/step - accuracy: 0.4905 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 36/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.5094 - loss: 0.6932\n",
      "Epoch 36: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 275ms/step - accuracy: 0.5090 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 37/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.4509 - loss: 0.6933\n",
      "Epoch 37: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 322ms/step - accuracy: 0.4512 - loss: 0.6933 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 38/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.4893 - loss: 0.6932\n",
      "Epoch 38: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 269ms/step - accuracy: 0.4894 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 39/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.5077 - loss: 0.6931\n",
      "Epoch 39: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 279ms/step - accuracy: 0.5075 - loss: 0.6931 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 40/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.4907 - loss: 0.6932\n",
      "Epoch 40: val_loss improved from 0.69315 to 0.69315, saving model to ./hello/.weights.h5\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 285ms/step - accuracy: 0.4907 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 41/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.4798 - loss: 0.6932\n",
      "Epoch 41: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.4800 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 42/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.4911 - loss: 0.6931\n",
      "Epoch 42: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 287ms/step - accuracy: 0.4910 - loss: 0.6931 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 43/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.4675 - loss: 0.6933\n",
      "Epoch 43: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 270ms/step - accuracy: 0.4676 - loss: 0.6933 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 44/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.4827 - loss: 0.6932\n",
      "Epoch 44: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 284ms/step - accuracy: 0.4829 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 45/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.4862 - loss: 0.6932\n",
      "Epoch 45: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.4861 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 46/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.4985 - loss: 0.6932\n",
      "Epoch 46: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 284ms/step - accuracy: 0.4985 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 47/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.5011 - loss: 0.6932\n",
      "Epoch 47: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 278ms/step - accuracy: 0.5011 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 48/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.5031 - loss: 0.6932\n",
      "Epoch 48: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 269ms/step - accuracy: 0.5031 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 49/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.4909 - loss: 0.6932\n",
      "Epoch 49: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 283ms/step - accuracy: 0.4908 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 50/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.4884 - loss: 0.6932\n",
      "Epoch 50: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 265ms/step - accuracy: 0.4884 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 51/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.5032 - loss: 0.6932\n",
      "Epoch 51: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 284ms/step - accuracy: 0.5030 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 52/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.4692 - loss: 0.6932\n",
      "Epoch 52: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 269ms/step - accuracy: 0.4693 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 53/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.4775 - loss: 0.6932\n",
      "Epoch 53: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 283ms/step - accuracy: 0.4775 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 54/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.5038 - loss: 0.6932\n",
      "Epoch 54: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 273ms/step - accuracy: 0.5038 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 55/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.5157 - loss: 0.6931\n",
      "Epoch 55: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 276ms/step - accuracy: 0.5154 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 56/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.4964 - loss: 0.6932\n",
      "Epoch 56: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 272ms/step - accuracy: 0.4962 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 57/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.4702 - loss: 0.6932\n",
      "Epoch 57: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.4705 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 58/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.4799 - loss: 0.6932\n",
      "Epoch 58: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 275ms/step - accuracy: 0.4800 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 59/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.4961 - loss: 0.6932\n",
      "Epoch 59: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 257ms/step - accuracy: 0.4961 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 60/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.4851 - loss: 0.6932\n",
      "Epoch 60: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 280ms/step - accuracy: 0.4851 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 61/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.4961 - loss: 0.6932\n",
      "Epoch 61: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.4961 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6932\n",
      "Epoch 62/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.5115 - loss: 0.6931\n",
      "Epoch 62: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 278ms/step - accuracy: 0.5113 - loss: 0.6931 - val_accuracy: 0.5006 - val_loss: 0.6932\n",
      "Epoch 63/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.5039 - loss: 0.6932\n",
      "Epoch 63: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 257ms/step - accuracy: 0.5038 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 64/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.4706 - loss: 0.6934\n",
      "Epoch 64: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 278ms/step - accuracy: 0.4708 - loss: 0.6934 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 65/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.5203 - loss: 0.6931\n",
      "Epoch 65: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 268ms/step - accuracy: 0.5200 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 66/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.5074 - loss: 0.6931\n",
      "Epoch 66: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 266ms/step - accuracy: 0.5073 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 67/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.4767 - loss: 0.6932\n",
      "Epoch 67: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 278ms/step - accuracy: 0.4767 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 68/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.5013 - loss: 0.6932\n",
      "Epoch 68: val_loss improved from 0.69315 to 0.69315, saving model to ./hello/.weights.h5\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 263ms/step - accuracy: 0.5013 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 69/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.4977 - loss: 0.6932\n",
      "Epoch 69: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 274ms/step - accuracy: 0.4976 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 70/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.4838 - loss: 0.6932\n",
      "Epoch 70: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 254ms/step - accuracy: 0.4839 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 71/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.5069 - loss: 0.6932\n",
      "Epoch 71: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 276ms/step - accuracy: 0.5068 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 72/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.5006 - loss: 0.6932\n",
      "Epoch 72: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 258ms/step - accuracy: 0.5006 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 73/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.4906 - loss: 0.6932\n",
      "Epoch 73: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 275ms/step - accuracy: 0.4905 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 74/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.5021 - loss: 0.6932\n",
      "Epoch 74: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 258ms/step - accuracy: 0.5020 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 75/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.4721 - loss: 0.6933\n",
      "Epoch 75: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 278ms/step - accuracy: 0.4722 - loss: 0.6933 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 76/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.5165 - loss: 0.6931\n",
      "Epoch 76: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 272ms/step - accuracy: 0.5162 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 77/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.4846 - loss: 0.6932\n",
      "Epoch 77: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 266ms/step - accuracy: 0.4847 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 78/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.4960 - loss: 0.6932\n",
      "Epoch 78: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 280ms/step - accuracy: 0.4959 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 79/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.4662 - loss: 0.6933\n",
      "Epoch 79: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 264ms/step - accuracy: 0.4663 - loss: 0.6933 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 80/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.4926 - loss: 0.6932\n",
      "Epoch 80: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 286ms/step - accuracy: 0.4925 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 81/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.4945 - loss: 0.6932\n",
      "Epoch 81: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 270ms/step - accuracy: 0.4946 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 82/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.4968 - loss: 0.6932\n",
      "Epoch 82: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 280ms/step - accuracy: 0.4967 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 83/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.4993 - loss: 0.6931\n",
      "Epoch 83: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.4991 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 84/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.5131 - loss: 0.6931\n",
      "Epoch 84: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 274ms/step - accuracy: 0.5129 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 85/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.4999 - loss: 0.6931\n",
      "Epoch 85: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 267ms/step - accuracy: 0.4998 - loss: 0.6931 - val_accuracy: 0.5006 - val_loss: 0.6932\n",
      "Epoch 86/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.4767 - loss: 0.6933\n",
      "Epoch 86: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 267ms/step - accuracy: 0.4765 - loss: 0.6933 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 87/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.5015 - loss: 0.6932\n",
      "Epoch 87: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 280ms/step - accuracy: 0.5015 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 88/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.4537 - loss: 0.6932\n",
      "Epoch 88: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 256ms/step - accuracy: 0.4541 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 89/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.4891 - loss: 0.6932\n",
      "Epoch 89: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 279ms/step - accuracy: 0.4891 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 90/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.4692 - loss: 0.6933\n",
      "Epoch 90: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 260ms/step - accuracy: 0.4693 - loss: 0.6933 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 91/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.4667 - loss: 0.6933\n",
      "Epoch 91: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 278ms/step - accuracy: 0.4669 - loss: 0.6933 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 92/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.4709 - loss: 0.6932\n",
      "Epoch 92: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 261ms/step - accuracy: 0.4710 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 93/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.4916 - loss: 0.6932\n",
      "Epoch 93: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 283ms/step - accuracy: 0.4917 - loss: 0.6932 - val_accuracy: 0.5006 - val_loss: 0.6931\n",
      "Epoch 94/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.4995 - loss: 0.6931\n",
      "Epoch 94: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 265ms/step - accuracy: 0.4994 - loss: 0.6931 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 95/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.5215 - loss: 0.6930\n",
      "Epoch 95: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 272ms/step - accuracy: 0.5211 - loss: 0.6930 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 96/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.4909 - loss: 0.6932\n",
      "Epoch 96: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 273ms/step - accuracy: 0.4910 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 97/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.4815 - loss: 0.6932\n",
      "Epoch 97: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 270ms/step - accuracy: 0.4815 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6932\n",
      "Epoch 98/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.4733 - loss: 0.6932\n",
      "Epoch 98: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 282ms/step - accuracy: 0.4731 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 99/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.5119 - loss: 0.6932\n",
      "Epoch 99: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.5117 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "Epoch 100/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.4984 - loss: 0.6932\n",
      "Epoch 100: val_loss did not improve from 0.69315\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 278ms/step - accuracy: 0.4982 - loss: 0.6932 - val_accuracy: 0.4994 - val_loss: 0.6931\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.5283 - loss: 0.6931\n",
      "Test Accuracy,50.07%\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    frame_feature_input = keras.Input((MAX_SEQ_LENGTH,NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,),dtype='bool')\n",
    "    \n",
    "    x = keras.layers.GRU(64,return_sequences=True)(frame_feature_input,mask=mask_input)\n",
    "    x =keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.GRU(32,return_sequences=True)(x)\n",
    "    x =keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.GRU(16)(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(8,activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(4,activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.1)(x)\n",
    "    output = keras.layers.Dense(len(class_vocab),activation='softmax')(x)\n",
    "    \n",
    "    rnn_model = keras.Model([frame_feature_input,mask_input],output)\n",
    "    \n",
    "    rnn_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return rnn_model\n",
    "\n",
    "def run_exp():\n",
    "    filepath = './hello/.weights.h5'\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath,save_weights_only=True,save_best_only=True,verbose=1)\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0],train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "    seq_model.load_weights(filepath)\n",
    "    _,accuracy = seq_model.evaluate([test_data[0],test_data[1]],test_labels)\n",
    "    print(f\"Test Accuracy,{round(accuracy*100,2)}%\")\n",
    "    return history,seq_model\n",
    "\n",
    "_,sequential_model = run_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: invalid escape sequence '\\F'\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\F'\n",
      "C:\\Users\\Sahu Suraj\\AppData\\Local\\Temp\\ipykernel_8476\\2713888675.py:25: SyntaxWarning: invalid escape sequence '\\F'\n",
      "  test_video = \"C:\\\\Users\\\\Sahu Suraj\\\\Videos\\\\New Project\\\\DeepFake Detection\\\\Check Data\\Fake1.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: C:\\Users\\Sahu Suraj\\Videos\\New Project\\DeepFake Detection\\Check Data\\Fake1.mp4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "    FAKE:50.06%\n",
      "    REAL:49.94%\n"
     ]
    }
   ],
   "source": [
    "from IPython import embed\n",
    "def prepare_single_video(frames):\n",
    "    frames=frames[None,...]\n",
    "    frame_mask = np.zeros(shape=(1,MAX_SEQ_LENGTH,),dtype='bool')\n",
    "    frame_features = np.zeros(shape=(1,MAX_SEQ_LENGTH,NUM_FEATURES),dtype='float32')\n",
    "    \n",
    "    for i,batch in enumerate(frames):\n",
    "        Video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH,Video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i,j,:] = feature_extractor.predict(batch[None,j,:])\n",
    "        frame_mask[i,:length] =  1\n",
    "    return frame_features,frame_mask\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "    frames = load_video(os.path.join(DATA_FOLDER,TRAIN_SAMPLE_FOLDER,path))\n",
    "    frame_features , frame_mask = prepare_single_video(frames)\n",
    "    probabilities= sequential_model.predict([frame_features,frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"    {class_vocab[i]}:{probabilities[i]*100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "test_video = \"C:\\\\Users\\\\Sahu Suraj\\\\Videos\\\\New Project\\\\DeepFake Detection\\\\Check Data\\Fake1.mp4\"\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[131, 136,  90],\n",
       "         [201, 220, 182],\n",
       "         [220, 252, 232],\n",
       "         ...,\n",
       "         [194, 207, 183],\n",
       "         [194, 207, 184],\n",
       "         [194, 207, 182]],\n",
       "\n",
       "        [[112, 127,  74],\n",
       "         [132, 151, 101],\n",
       "         [213, 236, 218],\n",
       "         ...,\n",
       "         [194, 207, 182],\n",
       "         [194, 207, 182],\n",
       "         [194, 207, 180]],\n",
       "\n",
       "        [[118, 124,  76],\n",
       "         [162, 169, 127],\n",
       "         [186, 203, 189],\n",
       "         ...,\n",
       "         [194, 207, 181],\n",
       "         [195, 208, 181],\n",
       "         [194, 208, 178]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[182, 213, 228],\n",
       "         [186, 214, 221],\n",
       "         [132, 149, 136],\n",
       "         ...,\n",
       "         [226, 244, 255],\n",
       "         [192, 206, 229],\n",
       "         [ 95, 105, 109]],\n",
       "\n",
       "        [[183, 214, 231],\n",
       "         [185, 215, 224],\n",
       "         [127, 148, 129],\n",
       "         ...,\n",
       "         [224, 243, 255],\n",
       "         [184, 205, 230],\n",
       "         [ 88, 114,  91]],\n",
       "\n",
       "        [[183, 214, 231],\n",
       "         [185, 215, 224],\n",
       "         [121, 142, 123],\n",
       "         ...,\n",
       "         [224, 243, 255],\n",
       "         [182, 205, 230],\n",
       "         [ 87, 115,  91]]],\n",
       "\n",
       "\n",
       "       [[[131, 136,  90],\n",
       "         [201, 220, 182],\n",
       "         [220, 252, 232],\n",
       "         ...,\n",
       "         [194, 207, 183],\n",
       "         [194, 207, 184],\n",
       "         [194, 207, 182]],\n",
       "\n",
       "        [[112, 127,  74],\n",
       "         [132, 151, 101],\n",
       "         [213, 236, 218],\n",
       "         ...,\n",
       "         [194, 207, 182],\n",
       "         [194, 207, 182],\n",
       "         [194, 207, 180]],\n",
       "\n",
       "        [[118, 124,  76],\n",
       "         [162, 169, 127],\n",
       "         [186, 203, 189],\n",
       "         ...,\n",
       "         [194, 207, 181],\n",
       "         [195, 208, 181],\n",
       "         [194, 208, 178]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[182, 213, 228],\n",
       "         [186, 214, 221],\n",
       "         [132, 149, 136],\n",
       "         ...,\n",
       "         [226, 244, 255],\n",
       "         [192, 206, 229],\n",
       "         [ 95, 105, 109]],\n",
       "\n",
       "        [[183, 214, 231],\n",
       "         [185, 215, 224],\n",
       "         [127, 148, 129],\n",
       "         ...,\n",
       "         [224, 243, 255],\n",
       "         [184, 205, 230],\n",
       "         [ 88, 114,  91]],\n",
       "\n",
       "        [[183, 214, 231],\n",
       "         [185, 215, 224],\n",
       "         [121, 142, 123],\n",
       "         ...,\n",
       "         [224, 243, 255],\n",
       "         [182, 205, 230],\n",
       "         [ 87, 115,  91]]],\n",
       "\n",
       "\n",
       "       [[[190, 167, 136],\n",
       "         [173, 151, 115],\n",
       "         [176, 152, 116],\n",
       "         ...,\n",
       "         [195, 207, 181],\n",
       "         [195, 208, 183],\n",
       "         [194, 207, 182]],\n",
       "\n",
       "        [[158, 136, 105],\n",
       "         [170, 148, 112],\n",
       "         [153, 141,  99],\n",
       "         ...,\n",
       "         [195, 207, 182],\n",
       "         [195, 208, 183],\n",
       "         [194, 207, 182]],\n",
       "\n",
       "        [[115,  95,  65],\n",
       "         [148, 138, 100],\n",
       "         [136, 136,  92],\n",
       "         ...,\n",
       "         [194, 207, 183],\n",
       "         [195, 208, 183],\n",
       "         [194, 207, 182]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[180, 215, 224],\n",
       "         [181, 211, 224],\n",
       "         [179, 211, 226],\n",
       "         ...,\n",
       "         [229, 244, 255],\n",
       "         [225, 240, 254],\n",
       "         [226, 244, 255]],\n",
       "\n",
       "        [[178, 212, 226],\n",
       "         [183, 209, 228],\n",
       "         [185, 210, 230],\n",
       "         ...,\n",
       "         [229, 246, 255],\n",
       "         [226, 247, 255],\n",
       "         [226, 247, 255]],\n",
       "\n",
       "        [[178, 212, 226],\n",
       "         [183, 208, 230],\n",
       "         [182, 206, 226],\n",
       "         ...,\n",
       "         [230, 247, 255],\n",
       "         [226, 247, 255],\n",
       "         [226, 247, 255]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[175, 188, 189],\n",
       "         [172, 185, 186],\n",
       "         [164, 178, 179],\n",
       "         ...,\n",
       "         [247, 250, 201],\n",
       "         [247, 254, 180],\n",
       "         [249, 254, 211]],\n",
       "\n",
       "        [[179, 193, 192],\n",
       "         [179, 192, 193],\n",
       "         [179, 196, 194],\n",
       "         ...,\n",
       "         [246, 252, 194],\n",
       "         [248, 255, 176],\n",
       "         [246, 254, 192]],\n",
       "\n",
       "        [[185, 199, 193],\n",
       "         [183, 199, 196],\n",
       "         [182, 198, 193],\n",
       "         ...,\n",
       "         [239, 255, 141],\n",
       "         [249, 254, 187],\n",
       "         [252, 252, 238]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 89,  67,  52],\n",
       "         [ 81,  61,  48],\n",
       "         [ 75,  54,  41],\n",
       "         ...,\n",
       "         [ 93,  94, 116],\n",
       "         [ 82,  86, 107],\n",
       "         [ 80,  85, 107]],\n",
       "\n",
       "        [[ 91,  72,  56],\n",
       "         [ 83,  62,  49],\n",
       "         [ 77,  56,  43],\n",
       "         ...,\n",
       "         [ 85,  92, 106],\n",
       "         [ 74,  80,  97],\n",
       "         [ 67,  73,  90]],\n",
       "\n",
       "        [[ 93,  74,  58],\n",
       "         [ 83,  62,  49],\n",
       "         [ 78,  57,  44],\n",
       "         ...,\n",
       "         [ 86,  95, 107],\n",
       "         [ 87,  96, 109],\n",
       "         [ 80,  86, 103]]],\n",
       "\n",
       "\n",
       "       [[[158, 169, 170],\n",
       "         [155, 168, 167],\n",
       "         [155, 168, 167],\n",
       "         ...,\n",
       "         [242, 254, 187],\n",
       "         [202, 237,  89],\n",
       "         [205, 242,  73]],\n",
       "\n",
       "        [[178, 190, 186],\n",
       "         [167, 178, 171],\n",
       "         [161, 175, 168],\n",
       "         ...,\n",
       "         [248, 254, 224],\n",
       "         [237, 251, 190],\n",
       "         [214, 244, 112]],\n",
       "\n",
       "        [[186, 198, 189],\n",
       "         [182, 194, 185],\n",
       "         [179, 193, 184],\n",
       "         ...,\n",
       "         [255, 254, 255],\n",
       "         [249, 253, 244],\n",
       "         [242, 253, 200]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 98,  82,  65],\n",
       "         [ 90,  71,  55],\n",
       "         [ 84,  63,  52],\n",
       "         ...,\n",
       "         [ 92,  94, 118],\n",
       "         [117, 121, 142],\n",
       "         [135, 142, 162]],\n",
       "\n",
       "        [[ 95,  79,  62],\n",
       "         [ 93,  72,  57],\n",
       "         [ 88,  66,  54],\n",
       "         ...,\n",
       "         [ 67,  70,  90],\n",
       "         [ 68,  71,  91],\n",
       "         [ 73,  79,  98]],\n",
       "\n",
       "        [[ 94,  78,  61],\n",
       "         [ 94,  73,  58],\n",
       "         [ 88,  66,  54],\n",
       "         ...,\n",
       "         [ 78,  81, 100],\n",
       "         [ 75,  78,  96],\n",
       "         [ 71,  77,  96]]],\n",
       "\n",
       "\n",
       "       [[[160, 174, 167],\n",
       "         [163, 176, 170],\n",
       "         [163, 177, 168],\n",
       "         ...,\n",
       "         [250, 255, 220],\n",
       "         [249, 255, 214],\n",
       "         [250, 253, 231]],\n",
       "\n",
       "        [[167, 179, 171],\n",
       "         [167, 181, 172],\n",
       "         [166, 182, 170],\n",
       "         ...,\n",
       "         [245, 255, 194],\n",
       "         [250, 253, 229],\n",
       "         [252, 253, 242]],\n",
       "\n",
       "        [[174, 186, 176],\n",
       "         [173, 185, 175],\n",
       "         [171, 185, 174],\n",
       "         ...,\n",
       "         [230, 251, 133],\n",
       "         [246, 253, 200],\n",
       "         [252, 251, 252]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[119, 104,  82],\n",
       "         [115, 100,  80],\n",
       "         [103,  88,  68],\n",
       "         ...,\n",
       "         [101, 111, 129],\n",
       "         [ 87, 101, 117],\n",
       "         [ 85,  98, 117]],\n",
       "\n",
       "        [[117, 102,  82],\n",
       "         [116, 100,  83],\n",
       "         [106,  91,  74],\n",
       "         ...,\n",
       "         [ 66,  74,  90],\n",
       "         [ 76,  84, 100],\n",
       "         [ 82,  90, 106]],\n",
       "\n",
       "        [[117, 102,  82],\n",
       "         [117, 101,  84],\n",
       "         [107,  92,  75],\n",
       "         ...,\n",
       "         [ 72,  81,  93],\n",
       "         [ 72,  79,  92],\n",
       "         [ 68,  72,  87]]]], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "sequential_model.save(\"BestModel.weights.h5\")\n",
    "sequential_model.save(\"BestModel.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
